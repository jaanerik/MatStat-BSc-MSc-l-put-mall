@ARTICLE{Parr.2019,
       author = {{Parr}, T. and {Markovic}, D. and {Kiebel}, S.J. and {Friston}, K.J.},
        title = "{Neuronal message passing using Mean-field, Bethe, and Marginal approximations}",
      journal = { Sci Rep 9, 1889},
     keywords = {Bayesian inference, Network models},
         year = "2019",
       adsurl = {https://www.nature.com/articles/s41598-018-38246-3#citeas}
}

@article{LYNGSO2002545,
title = {The consensus string problem and the complexity of comparing hidden Markov models},
journal = {Journal of Computer and System Sciences},
volume = {65},
number = {3},
pages = {545-569},
year = {2002},
note = {Special Issue on Computational Biology 2002},
issn = {0022-0000},
doi = {https://doi.org/10.1016/S0022-0000(02)00009-0},
url = {https://www.sciencedirect.com/science/article/pii/S0022000002000090},
author = {Rune B. Lyngsø and Christian N.S. Pedersen},
abstract = {The basic theory of hidden Markov models was developed and applied to problems in speech recognition in the late 1960s, and has since then been applied to numerous problems, e.g. biological sequence analysis. Most applications of hidden Markov models are based on efficient algorithms for computing the probability of generating a given string, or computing the most likely path generating a given string. In this paper we consider the problem of computing the most likely string, or consensus string, generated by a given model, and its implications on the complexity of comparing hidden Markov models. We show that computing the consensus string, and approximating its probability within any constant factor, is NP-hard, and that the same holds for the closely related labeling problem for class hidden Markov models. Furthermore, we establish the NP-hardness of comparing two hidden Markov models under the L∞- and L1-norms. We discuss the applicability of the technique used for proving the hardness of comparing two hidden Markov models under the L1-norm to other measures of distance between probability distributions. In particular, we show that it cannot be used for proving NP-hardness of determining the Kullback–Leibler distance between two hidden Markov models, or of comparing them under the Lk-norm for any fixed even integer k.}
}

@article{COX2019185,
title = {A factor graph approach to automated design of Bayesian signal processing algorithms},
journal = {International Journal of Approximate Reasoning},
volume = {104},
pages = {185-204},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18304298},
author = {Marco {Cox} and Thijs {van de Laar} and Bert {de Vries}},
keywords = {Probabilistic programming, Bayesian inference, Julia, Factor graphs, Message passing},
abstract = {The benefits of automating design cycles for Bayesian inference-based algorithms are becoming increasingly recognized by the machine learning community. As a result, interest in probabilistic programming frameworks has much increased over the past few years. This paper explores a specific probabilistic programming paradigm, namely message passing in Forney-style factor graphs (FFGs), in the context of automated design of efficient Bayesian signal processing algorithms. To this end, we developed “ForneyLab”2 as a Julia toolbox for message passing-based inference in FFGs. We show by example how ForneyLab enables automatic derivation of Bayesian signal processing algorithms, including algorithms for parameter estimation and model comparison. Crucially, due to the modular makeup of the FFG framework, both the model specification and inference methods are readily extensible in ForneyLab. In order to test this framework, we compared variational message passing as implemented by ForneyLab with automatic differentiation variational inference (ADVI) and Monte Carlo methods as implemented by state-of-the-art tools “Edward” and “Stan”. In terms of performance, extensibility and stability issues, ForneyLab appears to enjoy an edge relative to its competitors for automated inference in state-space models.}
}%

Cox, van de Laar, and de Vries. 2019. “A Factor Graph Approach to Automated Design of Bayesian Signal Processing Algorithms.” International Journal of Approximate Reasoning. 
@misc{Fox.2011,
      author       = {{Fox}, C.W. and Roberts, S.J},
      title        = "{A tutorial on variational Bayesian inference}",
      howpublished = "Artif Intell Rev 38, 85–95",
      year         = "2011",
      note         = "",
      annote       = ""
}

@misc{kuljus2022hybridclassifierspairwisemarkov,
      title={Hybrid classifiers of pairwise Markov models}, 
      author={Kristi Kuljus and Jüri Lember},
      year={2022},
      eprint={2203.10574},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2203.10574}, 
}

@misc{Soop.2023,
      author = {{Soop}, O},
      title = "{Kolmekaupa Markovi ahelate Viterbi raja lähendamine}",
      howpublished = "Tartu Ülikooli magistritöö",
      year         = "2023",
}

@misc{Bagaev.2022,
    author = {Bagaev, D.},
    title = "RxInfer.jl",
    year = {2022},
    publisher = {GitHub},
    howpublished = {\url{https://github.com/ReactiveBayes/ReactiveMP.jl/pull/169}}
}

@misc{Pihel.2024,
  author = {Pihel, J.E.},
  title = "{VBMC.jl}",
  year = {2024},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/jaanerik/VBMC.jl}},
  commit = {4f57d6a0e4c030202a07a60bc1bb1ed1544bf679},
}

@misc{Oja.1991,
      author = {{Oja}, E. and Oja, P.},

      title = "{Funktsionaalanalüüs}",
      howpublished = "Tartu Ülikool",
      year         = "1991",
}

@misc{Avans.2021,
      author = "{Avans, K.}",
      title = "{Paarikaupa Markovi mudel: definitsioon ja näited}",
      howpublished = "Tartu Ülikooli magistritöö",
      year         = "2021",
}

@misc{Koudahl.2024,
      author = "{Koudahl, M. T.}",
      title = "{A Factor Graph Approach to Active Inference}",
      howpublished = "Eindhoven University of Technology",
      year         = "2024",
}

@phdthesis{Beal2003VariationalAF,
  title={Variational algorithms for approximate Bayesian inference},
  author={Matthew J. Beal},
  year={2003},
  school = {University College London}
}

@book{lellep2013,
    author = {Lellep, J.},
    title = {Süsteemide optimeerimine},
    publisher = {Tartu Ülikooli Kirjastus},
    year = "2013",
}

@book{Parr.2022,
    author = {{Parr}, T. and Pezzulo, G. and Friston, K. J.},
    title = {Active Inference: The Free Energy Principle in Mind, Brain, and Behavior},
    publisher = {The MIT Press},
    year = {2022},
    month = {03},
    abstract = {The first comprehensive treatment of active inference, an integrative perspective on brain, cognition, and behavior used across multiple disciplines.Active inference is a way of understanding sentient behavior—a theory that characterizes perception, planning, and action in terms of probabilistic inference. Developed by theoretical neuroscientist Karl Friston over years of groundbreaking research, active inference provides an integrated perspective on brain, cognition, and behavior that is increasingly used across multiple disciplines including neuroscience, psychology, and philosophy. Active inference puts the action into perception. This book offers the first comprehensive treatment of active inference, covering theory, applications, and cognitive domains. Active inference is a “first principles” approach to understanding behavior and the brain, framed in terms of a single imperative to minimize free energy. The book emphasizes the implications of the free energy principle for understanding how the brain works. It first introduces active inference both conceptually and formally, contextualizing it within current theories of cognition. It then provides specific examples of computational models that use active inference to explain such cognitive phenomena as perception, attention, memory, and planning.},
    isbn = {9780262369978},
    doi = {10.7551/mitpress/12441.001.0001},
    url = {https://doi.org/10.7551/mitpress/12441.001.0001},
    eprint = {https://direct.mit.edu/book-pdf/2246566/book\_9780262369978.pdf},
}

@misc{Rowley.2024,
  author = {Rowley, C.},
  title = "{LogarithmicNumbers.jl}",
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub},
  howpublished = {\url{https://github.com/cjdoris/LogarithmicNumbers.jl}},
  commit = {4eb0546924607fc13d47c19e225ffb36de7a7591},
}
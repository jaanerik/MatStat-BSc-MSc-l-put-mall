\section{Variatsioonilised Bayesi meetodid Viterbi raja lähendamiseks}

\subsection{Paarikaupa ja kolmekaupa Markovi ahelad}

Et paarikaupa ja kolmekaupa Markovi ahelate omadusi on varasemalt uuritud palju \parencite{Soop.2023}, \parencite{Avans.2021}, \bla, siis toome välja vaid definitsioonid.

Olgu $(\mathcal{U},\sigma_u,\mu_u)$ ja $(\mathcal{X},\sigma_x,\mu_x)$ mõõduga ruumid. Hulgal $\mathcal{Z} \subseteq \mathcal{U} \times \mathcal{X}$, korrutis-$\sigma$ algebraga $\mathcal{B}(\mathcal{Z}) = \sigma_u \otimes \sigma_x$ ning korrutistõenäosusmõõduga $\mu_z = \mu_u \times \mu_x$ defineeritud homogeenset Markovi ahelat nimetatakse \textbf{paarikaupa Markovi ahelaks} (PMM).

Kui nüüd vaadelda eelnevatele mõõduga ruumidele lisaks veel tõenäosusruumi $(\mathcal{Y},\sigma_y,\mu_y)$, siis defineerime kolmekaupa Markovi ahela kui homogeense Markovi ahela tõenäosusruumil $(\mathcal{W}, \mathcal{B}(\mathcal{W}),\mu_w)$, kus $\mathcal{W} \subseteq \mathcal{U} \times \mathcal{X} \times \mathcal{Y}$, $\mathcal{B}(\mathcal{W}) := \sigma_u \otimes \sigma_x \otimes \sigma_y$ ja $\mu_w := \mu_u \times \mu_x \times \mu_y$.

Töös vaatleme vaid mudeleid üle lõplike $\mathcal{X},\mathcal{U}$ tähestike ehk diskreetsete $\sigma_u, \sigma_x$ algebrate ja loendavate mõõtudega, kuid viimane $\sigma_y$ võib olla suvaline. \bla midagi valesti!


\subsection{Variatsiooniline Bayesi meetod}

Olgu meil vaatlusandmeid kirjeldav juhuslik suurus $\bm{Y} \in \mathcal{Y}^T$ ning meid huvitavaid varjatud tunnuseid kirjeldav juhuslik suurus $\bm{X} \in \mathcal{X}^T$ ning loeme segavaks parameetriks juhusliku suuruse $\bm{U} \in \mathcal{U}^T$. Olgu teada homogeense kolmekordse Markovi ahela üleminekujaotus $P(u_k, x_k, y_k | u_{k-1}, x_{k-1}, y_{k-1})$ ning $P(u_1, x_1, y_1)$ iga $u_1 \in \mathcal{U}, x_1 \in \mathcal{X}, y_1 \in \mathcal{Y}$ jaoks. Saab näidata (peatükk \ref{chapter:forw_back}, valem \ref{eq:inhomog_markov}), et fikseeritud $\bm{y}$ korral $P(u_k, x_k | u_{k-1}, x_{k-1}) := P(u_k, x_k | u_{k-1}, x_{k-1}, \bm{y})$ on tegu mittehomogeense Markovi ahelaga.
    
Ülesandeks on leida $\bm{x} \in \mathcal{X}^T$, mis saavutab maksimumi suuruse 
$$\sum_{\bm{u} \in \mathcal{U}^T}{P(\bm{x}, \bm{u} | \bm{y})}$$
jaoks, kusjuures sellist $\mathbf{x}$ nimetatakse \textbf{Viterbi rajaks}. On võimalik näidata \parencite{LYNGSO2002545}, et see on NP raske ülesanne. Viterbi raja lähendamiseks kirjeldame variatsioonilisi meetodeid. Töö koosneb kahe algoritmi kirjeldamisest.
\begin{enumerate}
    \item Esimeses \emph{belief propagation} (BP) algoritmis leiame tõenäosusmõõdu $Q(\bm{u}, \bm{x}) = Q(\bm{x})Q(\bm{u})$, mis oleks hea lähend jaotusele $P(\bm{u},\bm{x} | \bm{y})$ KL kauguse mõttes, ning seejärel kasutame Viterbi algoritmi üle mõõdu $Q(\bm{x})$, et saada Viterbi raja hinnang $\bm{x}^*$.
    \item Teises \emph{variational message passing} (VMP) algoritmis nõuame ajalist sõltumatust tõenäosusmõõdus $Q(\bm{u}, \bm{x}) = \prod_{t=1}^T Q(x_t,u_t)$ ning samuti minimiseerime KL kaugust jaotuse $P(\bm{u},\bm{x} | \bm{y})$ suhtes. Viterbi raja lähendi leiame $x_t^* = \argmax_{x_t} \sum_{u_t} Q(x_t,u_t)$ abil iga $t$ korral.
\end{enumerate}

KL kauguse kuju on
$$KL[Q(\bm{u}, \bm{x}) \ ||\ P(\bm{u}, \bm{x} | \bm{y})] = \sum_{\bm{x} \in \mathcal{X}^T, \bm{u} \in \mathcal{U}^T} Q(\bm{u}, \bm{x}) \ln \frac{Q(\bm{u}, \bm{x})}{P(\bm{u}, \bm{x} | \bm{y})}.$$
Selline ebatavaline Kullback Leibler kauguse kuju, kus optimiseeritav jaotus on hoopis esimene argument, on tähtis, et meetod oleks arvutuslikult efektiivne. 

Edasi kirjutame 
\begin{align}
    \label{eq:kl}
    KL[Q(\bm{u}, \bm{x}) \ ||\ P(\bm{u}, \bm{x} | \bm{y})] &= - \sum_{\bm{u}, \bm{x}} Q(\bm{u}, \bm{x}) \left( \ln \frac{P(\bm{u}, \bm{x}, \bm{y})}{Q(\bm{u}, \bm{x})} - \ln P(\bm{y}) \right) \\
    \nonumber
    &= - \sum_{\bm{u}, \bm{x}} \left( Q(\bm{u}, \bm{x}) \ln \frac{P(\bm{u}, \bm{x}, \bm{y})}{Q(\bm{u}, \bm{x})}\right) - \ln P(\bm{y}).
\end{align}

Defineerime nüüd
\begin{equation}
\label{eq:elbo}
L[Q(\bm{u}, \bm{x})] = \sum_{\bm{u}, \bm{x}} \left( Q(\bm{u}, \bm{x}) \ln \frac{P(\bm{u}, \bm{x}, \bm{y})}{Q(\bm{u}, \bm{x})}\right)
\end{equation}

ning saame, et KL kauguse $KL[Q(\bm{u}, \bm{x}) || P(\bm{u}, \bm{x} | \bm{y})] = -L[Q(\bm{u}, \bm{x})] + \ln P(\bm{u}, \bm{x})$ minimiseerimiseks peame maksimiseerima suurust $L$, mille ingliskeelne lühend ELBO vastab mõistele \textit{evidence lower bound}. Kirjanduses kasutatakse ka mõistet \textit{variational free energy} sümboliga $F[Q, \bm{y}] := -L[Q, \bm{y}]$ \parencite{Parr.2019}. Funktsionaali L optimiseerimine võimaldab meil olla arvutuslikult efektiivsem, sest näiteks TMM mudelite puhul 
$$\ln P(\bm{u}, \bm{x}, \bm{y}) = \ln P(u_1, x_1, y_1) + \sum_{t=2}^T \ln P(u_t, x_t, y_t | u_{t-1}, x_{t-1}, y_{t-1})$$
on iga liidetav kergesti leitav, aga $\ln P(\mathbf{y})$ ei ole.

Avaldame fikseeritud $\bm{y} \in \mathcal{Y}^T$ jaoks
\begin{align}
    \label{eq:elbo1}
    L[Q(\bm{u}, \bm{x})] &=\sum_{\bm{u}, \bm{x}} Q(\bm{u}, \bm{x}) \ln P(\bm{u}, \bm{x}, \bm{y}) - \sum_{\bm{u}, \bm{x}} Q(\bm{u}, \bm{x}) \ln Q(\bm{u}, \bm{x})\\
    \label{eq:elbo2}
    &= \left< \ln P(\bm{u}, \bm{x},\bm{y}) \right>_{Q(\bm{u}, \bm{x})} + H(Q(\bm{u}, \bm{x})),
\end{align}
kus suurust $E = \ln P$ kutsutakse energiaks ning $H$ on Shannoni entroopia. Edaspidi kasutame ebatavaliselt keskväärtuse tähistust $\left<  \cdot \right>$ ka mõõtude puhul, mis ei pruugi olla tõenäosusmõõdud, nagu on seda tehtud teistes töödes \parencite{Parr.2019}. ELBO (\ref{eq:elbo2}) on töös kesksel kohal maksimiseeritav väärtus.

\subsection{Variatsiooniline meetod mõõdu parandamiseks}

Fikseerime $T \in \mathbb{N}$. Olgu $\mathcal{X}$ ja $\mathcal{U}$ lõplikud tähestikud ja $P(\bm{u},\bm{x})$ olgu mingi hulgal $\mathcal{X}^T,\mathcal{U}^T$ antud tõenäosusmõõt. Olgu $\mathcal{P}(\mathcal{U})$ ja $\mathcal{P}(\mathcal{X})$ vastavalt hulgal $\mathcal{U}^T$ ja $\mathcal{X}^T$ antud kõikide tõenäosusmõõtude hulk. Suvalise paari $Q_x \in \mathcal{P}(\mathcal{X})$ ja $Q_u \in \mathcal{P}(\mathcal{U})$ korral olgu $Q_u \times Q_x$ korrutismõõt hulgal $\mathcal{U}^T \times \mathcal{X}^T$. Vaatleme optimiseerimisülesannet
\begin{equation}
    \label{eq:problem}
    \min_{Q_u \times Q_x} D[Q_u \times Q_x || P],
\end{equation}
kus miinimum võetakse üle kõikide korrutismõõtude.

Näitame, et see miinimum leidub. Selleks piisab meenutada, et diskreetsete tõenäosusmõõtude kaalud moodustavad $|\mathcal{U}|^T-1$ ja $|\mathcal{X}|^T -1$ dimensionaalsed simpleksid, mis on kompaktsed ruumid ja mille otsekorrutis on samuti kompaktne ruum. Et KL kaugus on teise argumendi järgi pidev. Funktsionaalanalüüsist on teada Weierstrassi teoreem, (\cite{Oja.1991}{ II peatükk §5}) mille põhjal ekstreemumid ka saavutatakse. Veel enam, et KL kaugus on teise argumendi järgi ka rangelt kumer, on lahend ka ühene. Kas on vaja KL kauguse joint convexity tuletuskäiku? \bla

Järgmiseks uurime, kas mõõdu P marginaaljaotuste korrutis on ülesande (\ref{eq:problem}) lahendiks. Selleks piisab veenduda, et ühisjaotuse $ P = $ \bla korral.

Selgitame miks on $Q_u, Q_x$ leidmine ülesande (\ref{eq:problem}) puhul raske. Selleks uurime, et kui otsida määramata kordajate meetodi jaoks juhusliku suuruse $\mathbf{U} \sim Cat(q_1,\ldots,q_{|\mathcal{U}|^T})$ puhul kriitilist kohta, kus 
$$\frac{\partial}{\partial q_k} D[Q_u \times Q_x || P] = 0,$$
siis me saame leides tuletised võrrandi
$$\left< 1 + \ln q_k - \ln p(\bm{u}, k) \right>_{Q_u} = 0,$$
mis on meile probleemiks, sest $Q_u$ kaalud leiaksime analoogselt kasutades mõõtu $Q_x$. Sarnane probleem on motiveerivaks näiteks iteratiivse EM algoritmi tutvustamisel.

Seepärast otsime ka meie iteratiivset algoritmi, kus me valime algjaotused 
$$Q_u^{(0)} \in \mathcal{P}(\mathcal{U}), Q_x^{(0)} \in \mathcal{P}(\mathcal{X}), d_0 := D[Q_u^{(0)} \times Q_x^{(0)} || P]$$
ning seejärel leiame
\begin{align*}
    Q_u^{(i+1)} &= \argmin_{Q_u \in \mathcal{P}(\mathcal{U})} D[Q_u \times Q_x^{(i)} || P],& d_{2i -1} := D[Q_u^{(i+1)} \times Q_x^{(i)} || P] \\
     Q_x^{(i+1)} &= \argmin_{Q_x \in \mathcal{P}(\mathcal{X})} D[Q_u^{(i+1)} \times Q_x || P],& d_{2i} := D[Q_u^{(i+1)} \times Q_x^{(i+1)} || P].
\end{align*}

Et iga $i>0$ korral $Q_u^{(i)} \in \mathcal{P}(\mathcal{U})$, siis
$$ \argmin_{Q_u \in \mathcal{P}(\mathcal{U})} D[Q_u \times Q_x^{(i)} || P] \le D[Q_u^{(i)} \times Q_x^{(i)} || P]$$
ja sarnaselt saame arutleda ka $Q_x$ puhul. Seepärast on iteratsioonidel leitavad mõõdud sellised, mis moodustavad KL kauguste mõttes monotoonselt mittekasvava jada $(d_k)$. Et KL kaugus on mittenegatiivne, on see jada alt tõkestatud ning koondub. Kuid piirjaotused ei pruugi olla ülesande (\ref{eq:problem}) lahendiks. Kontranäide, kus on püsipunkt, aga ei ole lahend. Ilmselt, kui on lahend, siis on ka püsipunkt. Samuti Jüri rääkis lõplikust arvust sammudest püsipunkti jõudmisest - kas selle näitamine realistlik? \bla

Kas me 3.1.1 peatükis näitame, et kui me alustame mõõtudega, mis on Markovi omadusega, siis on ka uuel iteratsioonil selline omadus? Või on see lisaeeldus kuskil? \bla

Näitame, kuidas selline lokaalsesse miinimumi jõudmise algoritm käitub. Fikseerime $i$ korral $Q_x := Q_x^{(i)}$ ning minimiseerime funktsionaali $Q_u \mapsto D[Q_u \times Q_x || P]$:
\begin{align*}
     D[Q_u \times Q_x || P] &= \sum_{\bm{u},\bm{x}} Q_u(\bm{u}) Q_x(\bm{x}) \left( \frac{Q_u(\bm{u}) Q_x(\bm{x})}{P(\bm{u},\bm{x})} \right) \\
     &= -H[Q_x] + \sum_{\bm{u}}Q_u(\bm{u}) \ln Q_u(\bm{u}) - \sum_{\bm{u}}Q_u(\bm{u}) \ln P(\bm{u},\bm{x}),
\end{align*}
kus defineerime iga $\bm{u}$ korral
$$Q^*(\bm{u}) := \exp \left( \sum_{\bm{x}} Q_x(\bm{x}) \ln P(\bm{u},\bm{x}) \right).$$
Paneme tähele, et iga $\bm{u}$ korral $Q^*(\bm{u}) \in [0,1]$. Üldiselt ei ole mõõt $Q^*$ tõenäosusmõõt, aga näitame, et see ei ole nullmõõt, et me saaks hiljem seda normaliseerida. 

\subsection{Marginaalide lähendamine}

Uurime, kas me saame Viterbi rada leida, lähendades marginaale traditsioonilisema KL kauguse kuju $D[P || Q_u \times Q_x]$ minimiseerimisega. Näitame, et kuigi marginaliseerimine pole kitsendusteta üldiselt realistlik, siis nõudes eeldades $Q_u, Q_x$ Markovi omadust, on võimalik iteratiivne algoritm siiski luua.

Lahendame ülesannet
\begin{equation}
    \label{eq:alt_problem}
    \min_{Q_u \times Q_x} D[P || Q_u \times Q_x]
\end{equation}
ning näitame, et lahend tõepoolest on marginaaljaotuste korrutis. Selleks kirjutame
$$ D[P || Q_u \times Q_x] = \sum_{\bm{u}, \bm{x}} P(\bm{u},\bm{x}) \left( \ln P_x(\bm{x}) - \ln Q_u(\bm{u}) - \ln Q_x(\bm{x}) \right) $$
ja otsime nt minimiseerivat $Q_x$. Kirjutame
$$
-\sum_{\bm{u}, \bm{x}} P(\bm{u},\bm{x}) \ln Q_x(\bm{x}) = -\sum_{\bm{x}} P_x(\bm{x}) \frac{\ln Q_x(\bm{x}) P_x(\bm{x})}{P_x(\bm{x})} = H[P_x] + KL[P_x||Q_x]
$$
ja et KL kaugus on $0$ parajasti siis, kui kaks jaotust on võrdsed, saamegi ülesande (\ref{eq:alt_problem}) lahendiks $Q_x$ marginaaljaotuse $P_x$. Argumenteerime $Q_u$ puhul analoogselt. 

Järgmisena näitame, et kuigi nii on teoreetiliselt võimalik marginaale leida, ei ole need üldiselt meie mudelites Markovi omadusega, mispärast ei saa rakendada Viterbi algoritmi Viterbi raja leidmiseks, ja ülesanne on endiselt NP raske. \bla



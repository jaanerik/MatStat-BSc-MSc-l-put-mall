\section{Viterbi raja lähendamine}

Nüüdseks oleme tutvunud paarikaupa Markovi ahelatel (\ref{eq:pairwise_markov}) Viterbi raja probleemiga (\ref{problem_def}) ja õigustanud iteratiivset lahendusviisi peatükkides \ref{sec:theory_variational_method}, \ref{sec:theory_approximating_marginals}, mille abil luua algoritm ülesande lähendi leidmiseks. Eelmises peatükis tutvustasime tuntud algoritme meie ülesande kontekstis ning nüüd implementeerime kaks algoritmi.

Mõlema puhul vaatleme diskreetset mittehomogeenset paarikaupa Markovi ahelat $\{U_t,X_t\}$ jaotusega $p$, mida kirjeldab \ref{eq:pairwise_markov}, kus algjaotus on $\pi$ ning üleminekumaatriks ajahetkel $t$ on $p_t$ ehk paaride $(u_{t-1},x_{t-1}),(u_t,x_t) \in \mathcal{U} \times \mathcal{X}$ korral on üleminekutõenäosus $p_t(u_t,x_t|u_{t-1},x_{t-1})$. Soov on leida jaotus $q$, mille abil leida Viterbi raja lähend $\bm{x}^*$. Esimeses algoritmis eeldame sõltumatust $q(\bm{u},\bm{x}) = q_u(\bm{u})q_x(\bm{x})$ ning teises $q(\bm{u},\bm{x}) = \prod_{t=1}^T q_t(u_t,x_t)$.


\subsection{Belief propagation (BP)}\label{sec:BP}

% {Tuleme aga enne veel tagasi peatükis \ref{sec:theory_variational_method} tõstatatud teema juurde.} \textcolor{red}{See lause kõlab nagu poliitiku sõnavõtt}. Kui leiduvad sellised $u_t,x_t,u_{t-1},x_{t-1}$, et $p(u_t,x_t|u_{t-1},x_{t-1}) > 0$ ja $q(x_t|x_{t-1}) q(u_t|u_{t-1})>0$, siis KL kaugus ja funktsionaal ELBO (\ref{eq:elbo}) ei ole lõplikud ja me peame iteratsioonil optimiseeritava variatsioonilise mõõdu puhul uuendama $q_x(x_t|x_{t-1}) = 0$ või $q_u(u_t|u_{t-1})=0$.

% \textcolor{red}{Igas peatükis oma tähistus! Kui meenutad jaotust 1.3, võiks kasutada samu tähistusi. Miks mõnikord on $p$ (näiteks $p(u_t,x_t|u_{t-1},x_{t-1})$ ja mõnikord $q$ (näiteks $q(x_t|x_{t-1})$. Mis on $p$ ja $q$ vahe ja mille poolest nad erinevad $P$-st ja $Q$-st?
% \\\\
% See pikk lause ise ka on väga segane -- miks ei ole (1.3) lõplik? Mida tähendab "me peame iteratsioonil optimiseeritava variatsioonilise mõõdu puhul uuendama $q_x(x_t|x_{t-1}) = 0$ või $q_u(u_t|u_{t-1})=0$"? Mis asjad on variatsioonilised mõõdud???? }


% \textcolor{red}{esimene võrdus on vale, peab olema $p(u_{t},x_{t}) = P(U_{t} = u_{t}, X_{t} = x_{t}| \bm{Y}=\bm{y})$, teisest ei saa aru, mida mõtled. }

Me oleme näidanud, milline on tõenäosusmõõdu 
$$q_u \times q_x: \mathcal{U}^T \times \mathcal{X}^T \ni (\bm{u},\bm{x}) \mapsto q_u(\bm{u}) q_x(\bm{x}) \in [0, \infty)$$ 
puhul puhul iteratiivse algoritmi samm (\ref{eq:general_update}) minimiseerimaks tagurpidist KL kaugust $ \DKL[q_u \times q_x \| p]$. Näitame, kuidas käib sellise algoritmi implementeerimine, kui $p$ on mittehomogeenne diskreetne paarikaupa Markovi ahel.

Vaatleme tõenäosusmõõtu $q_u^{(0)}$, kus iga vektori $\bm{u} \in \mathcal{U}^T$ korral kehtib (\ref{eq:almost_markovian}) ehk
\begin{equation*}
    q(\bm{u})^{(0)} = \pi_u^{(0)}(u_1) \prod_{t=2}^T \Tilde{q}_t^{(0)}(u_t|u_{t-1}),
\end{equation*}
kusjuures mõõt $\Tilde{q}_t^{(0)}(\cdot|u_{t-1})$ ei pruugi olla tõenäosusmõõt. Aga meenutame, et me oskame leida $q_u^{(0)}(u_t)$ iga $t$ korral ja $q_u^{(0)}(u_{t-1},u_t)$ iga $t>2$ korral \eqref{eq:fb1} ja \eqref{eq:fb2} abil.
% \textcolor{red}{Siin peaks ikka natuke täpsemalt lahti seletama, mis on Markovi ahel $q$ (et $q(\bm{u})=q(u_1)q(u_2|u_1)\cdots q(u_T|u_{T-1}$) jne.)}
% \\\\
Kirjutame variatsioonilise Bayesi iteratsiooni vastavalt tulemusele (\ref{eq:general_update}) ja saame
% \textcolor{red}{Siin $q(x)$ pole tõenäosusmõõt, $q(u)$ jällegi on. Võibolla pole siinkohal $q(x)$-i defineerida vaja. Või tähistada kuidagi teisiti. Või lisada, et pole tõenäosusmõõt.}
\begin{align*}
    \ln q_x^{(0)}(\bm{x}) &= \sum_{\bm{u}} \ln p(\bm{u},\bm{x}) q_u^{(0)}(\bm{u}) + \ln Z_x^{(0)}
\end{align*}
ning näitame, et tõenäosusmõõt $q_x^{(0)}$ on võimalik samuti esitada sarnaselt mõõdule $q_u$ kujul (\ref{eq:almost_markovian}). Uurime eelmise avaldise normaliseerimata paremat poolt
\begin{align*}
    \sum_{\bm{u}} \ln p(\bm{u},\bm{x}) q_u^{(0)}(\bm{u}) &=  \sum_{\bm{u}} \ln \left( \pi(x_1, u_1) \prod_{t=2}^T p_t(x_t,u_t | x_{t-1}, u_{t-1}) \right) q_u^{(0)}(\bm{u}) \\
    &= \sum_{u_1} \ln \pi(x_1, u_1) q_u^{(0)}(u_1)  + \sum_{t=2}^T \sum_{\bm{u}_{t-1,t}} \ln p_t(u_t,x_t | u_{t-1}, x_{t-1}) q_u^{(0)}(u_{t-1},u_{t}).
\end{align*}

Defineerime iga $t$ ning iga $x_t \in \mathcal{X}$ korral $\ln \pi_x^{(0)}(x_1)$ ja $\ln q_x^{(0)}(x_t | x_{t-1})$, kus
$$\ln \pi_x^{(0)}(x_1) := \sum_{u_1 \in \mathcal{U}}  q_u^{(0)}(u_1) \ln p(u_1,x_1)$$
ning iga $t > 1$ ja iga $(x_{t-1}, x_t) \in \mathcal{X}^2$ jaoks
$$\ln q_x^{(0)}(x_{t}|x_{t-1}) := \sum_{u_{t-1,t} \in \mathcal{U}^2}  q_u^{(0)}(u_{t-1}, u_t) \ln p_t(u_{t},x_t | u_{t-1},x_{t-1}).$$ 
Saame tõenäosusmõõdu
\begin{equation}
    \label{eq:update_is_still_markov}
    q_x^{(0)}(\bm{x}) = \frac{1}{Z_x^{(0)}}\pi_x^{(0)}(x_1)\prod_{t=2}^T q_x^{(0)}(x_t | x_{t-1}),
\end{equation}
kus $Z_x$ on kergesti leitav (\ref{eq:almost_markovian_normalisation}) abil.
% \textcolor{red}{Kas edasi-tagasi valemid   \eqref{eq:fb1} ja \eqref{eq:fb2} (need tuleb sul muidugi korda teha)  kehtivad ka siis, kui $q(x)$ pole  tõenäosusmõõt? Argumenteeri.}
% \\\\

Nüüd iteratsiooni teisel sammul leiame
% \textcolor{red}{Iteratsioonil on sammud, mitte osad}
\begin{align*}
    q^{(1)}(\bm{u}) = \frac{1}{Z_u^{(1)}}\pi_u^{(1)}(u_1) \prod_{t=2}^n q_u^{(1)}(u_t | u_{t-1}),
\end{align*}
kus 
\begin{align*}
    \ln \pi_u^{(1)}(u_1) :&= \sum_{x_1 \in \mathcal{X}}  \pi_x^{(0)}(x_1) \ln \pi(x_1,u_1) \\
    \ln q_u^{(1)}(u_{t}|u_{t-1}) :&= \sum_{\bm{x}_{t-1,t} \in \mathcal{X}^2}  q_x^{(0)}(x_{t-1}, x_t) \ln p_t(x_{t},u_t | x_{t-1},u_{t-1})
\end{align*}
ja $Z_u^{(1)}$ on (\ref{eq:almost_markovian_normalisation}) abil leitav normaliseeriv konstant.

Seda saime teha analoogselt, sest kui $q_u^{(0)}$ on kujul (\ref{eq:almost_markovian}), siis on ka (\ref{eq:update_is_still_markov}) põhjal $q_x^{(0)}$ ja $q_u^{(1)}$ Markovi ahelad jne. 
% \textcolor{red}{Tähistused: kas $q_x(x)$ ja $q(x)$ on sama? kui ja, siis miks paralleelsed tähistused? Võibolla oleks parem iteratsiooni sammude indekseid $q^{(i)}$ kogu aeg juures hoida, et oleks selge. Ja kui $q_x$ pole tõenäosusmõõt (pole ju!), kas ta siis saab olla Markovi mõõt?}
% \textcolor{red}{Kas see $q$ valemis (\ref{eq:is_markovian}) on $q(\bm{x}) = r(x_1)\prod_{t=2}^T r(x_t | x_{t-1})$? Kui tõenäosusmõõt on sellisel kujul, siis ta on Markovi mõõt, seda pole vaja tõestada. Peale selle on sul argumendid vales järjekorras -- defineerid $q(x)$ ja siis väidad, et edasi-tagasi valemitega saad arvutada $q(x_t)$ jne. Aga edasi-tagasi valemid juba eeldavad Markovi omadust, sest muidu nad ei kehti. 
% \\\\
% }
Seega sobivatel algväärtustustel, kus initisialiseerime $q_u^{(0)}$ Markovi ahelana, ei toimu optimiseerimine kitsendusega üle Markovi ahelate, vaid üle kõikide mõõtude $\mathcal{P}(\mathcal{X})$.  
% \textcolor{red}{Tähistused -- kust see $Q$ nüüd tuleb????}

Peatükis \ref{sec:theory_variational_method} kirjeldatud ja siin peatükis implementeeritud iteratiivne algoritm tagab iteratsioonidel KL kaugust minimiseeriva korrutismõõdu $q_u \times q_x$. 

Viterbi raja lähendi ehk lähendi ülesandele (\ref{problem_def}) anname kui Viterbi raja üle leitud mõõdu $q_x$
$$\argmax_{\bm{x}} q_x(\bm{x}).$$

% \textcolor{red}{Kas $q$ või $q_x$? Järbmine lause on väga segane -- mis on eksperiment? Kes mida loodab? Kas on mingi teoreetiline argument/põhjendus, et mõõtu $q_x$ (see on VB iteratsiooni lõpp-produkt) maksimiseeriv rada on Viterbi rada? Ega ikka pole küll. } 

Loodud algoritm on hea, kui piisavalt paljude  mittehomogeensete diskreetsete paarikaupa Markovi ahelate $p(\bm{u},\bm{x})$ korral
$$ \argmax_{\bm{x}} q_x(\bm{x}) = \argmax_{\bm{x}} \sum_{\bm{u}} p(\bm{u},\bm{x}). $$

\subsection{Variational Message Passing (VMP)}\label{sec:VMP}

Vaatleme mittehomogeenset diskreetset paarikaupa Markovi ahelat $\{U_t,X_t\}_t$ jaotusega $p$ (\ref{eq:pairwise_markov}). 
% Juhuslikule suurusele vastava variatsioonilise mõõdu uuendamiseks on algoritmis tarvis vaid Markovi ümbruses olevate juhuslike suuruste variatsioonilisi jaotusi. See on klassikalisem näide ICM-ist e \emph{iterated conditional mode}'ist, kusjuures on võimalik algoritmis ka tööd paralleliseerida. \textcolor{red}{Kahest eelmisest lausest ei saa aru.}

% \textcolor{red}{Kas  $q(\bm{u}, \bm{x})$  on sama, mis  $Q(\bm{u}, \bm{x})$ peatükis 1? Et nüüd vaatad olukorda, kus $Q= \prod_{t=1}^T{Q(u_t, x_t)}$. Kas $w_t=(u_t,x_t)$? Kas enne polnud mitte $z_t$? Kas on vaja tuua veel uusi tähistusi? Saan aru, et $Q(w)$ on kogu aeg sama mõõt ega sõltu $t$-st. Või sõltub, st $Q_t(w_t)$? See on väga oluline erinevus, sest kui $Q$ on iga $t$ korral sama, on Viterbi lähend konstantne.}
Eeldame, et $q(\bm{u}, \bm{x}) = \prod_{t=1}^T{q_t(u_t, x_t)}$, kus iga $q_t(u_t, x_t)$ on tõenäosusmõõt. Fikseerime nüüd $t$. Meid huvitav jaotus on $q_t(u_t, x_t)$ ning jaotuse
$$q_{\setminus t} = q_1 \times \ldots \times q_{t-1} \times q_{t+1} \times \ldots \times q_T$$
loeme fikseerituks. Sellisel juhul on KL kauguse (\ref{eq:kl_def}) minimiseerimine ekvivalentne suuruse $\DKL^{t}[q_t \times q_{\setminus t} \| p]$ minimiseerimisega, kus
\begin{align*}
    \DKL^{t}[q_t \times q_{\setminus t} \| p] :=& \sum_{\substack{u_{t+1}, x_{t+1}\\ u_{t}, x_t}} q_t(u_{t},x_t) q_{t+1}(u_{t+1},x_{t+1}) \Big(\ln \frac{p_{t+1}(u_{t+1},x_{t+1} \ |\ u_{t}, x_t )}{q_t(u_{t},x_t)q_{t+1}(u_{t+1}, x_{t+1})}  \Big)\\
    +&\sum_{\substack{u_{t}, x_t \\ u_{t-1}, x_{t-1}}} q_{t-1}(u_{t-1},x_{t-1}) q_t(u_{t},x_t) \Big(\ln \frac{p_t(u_{t}, x_t\ |\ u_{t-1}, x_{t-1})}{q_{t-1}(u_{t-1},x_{t-1})q_t(u_{t},x_t)}\Big),
\end{align*}
kusjuures $\DKL^{1},\DKL^{T}$ korral vaatleme vastavalt vaid teist ja esimest liidetavat. Kaks probleemi on ekvivalentsed, sest me saame muuhulgas $p$ Markovi omadust kasutades kirjutada KL kauguse (\ref{eq:kl_def}) ülejäänud liidetavad konstandina $c$, kus
$$\DKL[q_t \times q_{\setminus t} \| p] = \DKL^t[q_t \times q_{\setminus t} \| p] + c$$
ning optimaalne $q_t$ on nii $\DKL[q_t \times q_{\setminus t} \| p]$ kui $\DKL^t[q_t \times q_{\setminus t} \| p]$ jaoks sama.

Saame kirjutada iteratsioonisammu (\ref{eq:vmp_update}) kui 
\begin{equation}
    \label{eq:vmp_implement}
    \Bar{q}_t(u_t,x_t) = \frac{ q_t^*(u_t,x_t)}{Z_t},
\end{equation}
kus
\begin{align*}
    \ln q_t^*(u_t,x_t)&=\sum_{u_{t+1},x_{t+1}}\ln p_{t+1}(u_{t+1},x_{t+1}|u_t,x_t)q_{t+1}(u_{t+1},x_{t+1}) \\
    &+ \sum_{u_{t-1},x_{t-1}}\ln p(u_{t},x _t|u_{t-1}, x_{t-1})q_{t-1}(u_{t-1}, x_{t-1})\\
    Z_t :&= \sum_{u_t,x_t} q_t^*(u_t,x_t)
\end{align*}



% \textcolor{red}{Mis on $p(w_{1:T})$? Kui $p(\bm{u}, \bm{x})$ on tinglik jaotus nii nagu ptk-s 3.1.1., siis ülaltoodud valem pole (1.3) vaid miinusmärgiga (1.2). Teiseks -- kolmikuid pole vaja, piisab paarikaupa summadest:
% $$\sum_{w_t,w_{t-1}} q(w_t)q(w_{t-1})\ln {P(w_t|w_{t-1})\over q(w_t)}$$
% Siin $P(w_t|w_{t-1})$ on iga $t$ korral erinev. Edasi argumenteeri nagu esimeses peatükiski, et  iga $t$ korral on optimaalne mõõt $q^*(w_t)$ (tegelikult küll $q_t^*(w_t)$) järgmine
% $$\ln q^*(w_t)=\sum_{w_{t+1}}\ln p(w_{t+1}|w_t)q^*(w_{t+1})+
% \sum_{w_{t-1}}\ln p(w_{t}|w_{t-1})q^*(w_{t-1})+c_t.$$
% Ja siit siis iteratiivne algoritm nagu peatükis \ref{sec:theory_variational_method}. Näita, et igal sammul vähendab KL kaugust (nagu peatükis \ref{sec:theory_variational_method}).
% Äkki see osa sobiks ka esimesse peatükki? Edasine "message passing" jne on lihtsa asja üleliia keerulisemaks ajamine ja seda pole vaja. Lõpuks, kui algoritm on koondunud/töö lõpetanud, on $n$ mõõtu 
% $q_1(w_1),\ldots,q_n(w_n)$ ja siis on lihtne iga $t$ korral $u_t$ välja summeerida ja üle $x_t$ maksimiseerida. Simulatsioonidega võrrelda selle algoritmi käitumist eelmisega.}

Algoritmi implementeerides saame initsialiseerida mõõdud $q_t^{(0)}$ ühtlase jaotusena üle tähestiku $\mathcal{U} \times \mathcal{X}$ iga $t$ korral. Eksperimentaalosas saame muuhulgas uurida, kas algoritm on erinevate algväärtustuste osas tundlik. Iteratsioonil $i$ me uuendame (\ref{eq:vmp_implement}) abil järjestikku $t = 1, \ldots, T$ korral mõõtu $q_t^{(i)}(u_t,x_t) = \Bar{q}_t(u_t, x_t)$. Tasub tähele panna, et me kasutame $q_t^{(i)}$ leidmiseks mõõte $q_{t-1}^{(i)}$ ja $q_{t+1}^{(i-1)}$.

Kui algoritm on meie hinnangul koondunud, anname Viterbi raja lähendi pärast $N$ iteratsiooni mõõtude $q_1^{(N)},\ldots,q_T^{(N)}$ jaoks kui $(x^*_t)_{t=1}^T$, kus
$$x^*_t = \argmax_{x_t} \sum_{u_t} q_t^{(N)}(u_t, x_t)$$
ning simulatsioonide abil saame võrrelda selle algoritmi käitumist peatükis \ref{sec:BP} kirjeldatud algoritmiga.

% \subsubsection{VMP rakendamine teegis RxInfer kolmekaupa Markovi ahelal} 

% Eeldame, et me tähistame sõnu indeksitena $w_t \in \{1,\ldots,|\mathcal{W}|\}$ ja saame tähistada $w_t \sim Cat(G)$ korral $G_{w_t} = q(w_t)$, mis aitab meil kirjeldada jaotusi vektorkujul. Kasutame samu tähistusi nagu seda tehti \cite{Beal2003VariationalAF}{ kolmandas peatükis}. Kirjutame mittehomogeense paarikaupa Markovi ahela $(W_t)_{t=1}^T = (U_t, X_t)_{t=1}^T$ alg- ja üleminekumaatriksid kui
% \begin{align*}
%    A_t &= \{a^t_{jj'} | a^t_{jj'} = p(w_t = j | w_{t-1} = j')\} &\sum_{j'=1}^{|\mathcal{W}|} a^t_{jj'} = 1 \; \forall j, t\\
%     \pi &= \{ \pi_j | \pi_j = p(w_1 = j) \} &\sum_{j=1}^{|\mathcal{W}|} \pi_{j} = 1.
% \end{align*}

% \begin{figure}[!ht]
% \centering
% \resizebox{1\textwidth}{!}{%
% \begin{circuitikz}
% \tikzstyle{every node}=[font=\LARGE]

% \draw  (0,19) rectangle  node {\LARGE $\pi$} (2,17);
% \draw [->, >=Stealth] (2,18) -- (3.75,18)node[pos=0.5,above, fill=white]{$w_{1}$};
% \node [font=\Large] at (1.5,16) {};
% \draw [->, >=Stealth] (9.5,18) -- (11.25,18)node[pos=0.5,above, fill=white]{$w_{T-1}$};
% \node [font=\Large] at (5,16.25) {};
% \draw  (3.75,19) rectangle  node {\LARGE $A_2$} (5.75,17);
% \draw  (11.25,19) rectangle  node {\LARGE $A_T$} (13.25,17);
% \draw [dashed] (5.75,18) -- (7.5,18);
% \node [font=\Large] at (12,16.25) {};
% \node [font=\Large] at (12,16.25) {};

% \draw  (7.5,19) rectangle  node {\LARGE $A_{T-1}$} (9.5,17);
% \draw [->, >=Stealth] (13.25,18) -- (15,18)node[pos=0.5,above, fill=white]{$w_{T}$};
% \draw  (15,19) rectangle  node {\LARGE $Id$} (17,17);
% \end{circuitikz}
% }%
% \caption{Forney stiilis graaf mudelist (\ref{eq:model2_1}), (\ref{eq:model2_2}).}
% \label{fig:tmm_model2}
% \end{figure}

% Fikseerime $t \in \{2,\ldots,T\}$ ning kirjutame vektorkujul $t-1$ ja $t$ jaotuse kui
% \begin{align*}
%     q(w_{t-1}) &\sim Cat(F),\\
%     q(w_{t+1}) &\sim Cat(G).
% \end{align*}
% Saame kirjeldada sõnumeid kui 
% \begin{align*}
%     \ln \vec{\nu}(w_t) \propto \ln \vec{\eta}(w_t) :&= \sum_{w_{t-1}} q(w_{t-1}) \ln P(w_t | w_{t-1}) = (\ln A \times F)_{w_t} \\
%     \ln \cev{\nu}(w_{t-1}) \propto \ln \cev{\eta}(w_{t-1}) :&= \sum_{w_{t+1}} q(w_{t}) \ln P(w_{t} | w_{t-1}) = ((\ln A)^T \times G)_{w_{t}},
% \end{align*}
% kus variatsioonilised sõnumid $\vec{\nu}_{t}(w_t), \cev{\nu}_{t}(w_{t-1})$ on normaliseeritud. Nüüd saame kirjutada tulemuse kui $q(w_t) \sim Cat(H)$. Saame nüüd leida tipu $A_t$ keskmise energia

% $$\sum_{\bm{w}_{t-1,t}} q(w_{t-1})q(w_t) \ln p(w_t|w_{t-1}) = F^T \times \ln A_t \times H.$$

% Et seda modelleerida teegis RxInfer, on viimaks tarvis implmenteerida ühiktipp (joonisel \ref{fig:tmm_model2} kui \emph{Id}), mis edastab ühtlase jaotusega sõnumi servale $w_T$. Siinkohal on analoog BP algoritmis $\beta(u_T), \beta(x_T)$ võrdsustamine ühega. See on tarvilik, sest RxInfer vajab serva defineerimiseks mõlema tipu täpsustamist.

% % 